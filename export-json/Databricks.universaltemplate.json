{
    "agent": null,
    "agentCluster": null,
    "agentClusterVar": null,
    "agentFieldsRestriction": "No Restriction",
    "agentType": "Any",
    "agentVar": null,
    "automaticOutputRetrievalFieldsRestriction": "No Restriction",
    "broadcastCluster": null,
    "broadcastClusterVar": null,
    "createConsole": false,
    "credentialFieldsRestriction": "No Restriction",
    "credentials": null,
    "credentialsVar": null,
    "description": "This universal is to integrate databricks with Universal controller ",
    "desktopInteract": false,
    "elevateUser": false,
    "environment": [],
    "environmentVariablesFieldsRestriction": "No Restriction",
    "exitCodeOutput": null,
    "exitCodeProcessing": "Success Exitcode Range",
    "exitCodeProcessingFieldsRestriction": "No Restriction",
    "exitCodeText": null,
    "exitCodes": "0",
    "fields": [
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Text Field 1",
            "fieldType": "Text",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "specify the Databricks URL",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Databricks URL",
            "name": "data_bricks_url",
            "noSpaceIfHidden": false,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": false,
            "required": true,
            "sequence": 0,
            "showIfField": null,
            "showIfFieldValue": null,
            "sysId": "fafeff84d962407cb3977c6248a5af44"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Credential Field 1",
            "fieldType": "Credential",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Provide the Databricks Personal token or the Azure AD token",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Bearer Token",
            "name": "bearer_token",
            "noSpaceIfHidden": false,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": false,
            "required": true,
            "sequence": 1,
            "showIfField": null,
            "showIfFieldValue": null,
            "sysId": "9f6274ce78434f369ee43cd0ea7e3039"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [
                {
                    "fieldValue": "create_job",
                    "fieldValueLabel": "Create Job",
                    "sequence": 0,
                    "sysId": "8cb2b93405224884b2d8ccdd8b84f77a",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "list_jobs",
                    "fieldValueLabel": "List Jobs",
                    "sequence": 1,
                    "sysId": "a4d05eb3faac4c20a61503a1383b2d0d",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "delete_job",
                    "fieldValueLabel": "Delete Job",
                    "sequence": 2,
                    "sysId": "22dfd0cc4a504001b25680cf555adb8e",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "get_job_details",
                    "fieldValueLabel": "Get Job Details",
                    "sequence": 3,
                    "sysId": "43253e4dd1c444de809f1573dcf26316",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "run_now_job",
                    "fieldValueLabel": "Run Now Job",
                    "sequence": 4,
                    "sysId": "e7bc881a572048c99cb2b26ac1a367c3",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "run_submit_job",
                    "fieldValueLabel": "Runs Submit Jobs",
                    "sequence": 5,
                    "sysId": "7681d3aa0c5943e88dd7bbb8ad7afa31",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "cancel_job_run",
                    "fieldValueLabel": "Cancel Run Job",
                    "sequence": 6,
                    "sysId": "07aed108011c42c5a34466c9652bd357",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "create_cluster",
                    "fieldValueLabel": "Create Cluster",
                    "sequence": 7,
                    "sysId": "3f8526402b514ca980026c8509ca53fa",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "start_cluster",
                    "fieldValueLabel": "Start Cluster",
                    "sequence": 8,
                    "sysId": "70fcff4b65ff46a6b2ea282162407c96",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "restart_cluster",
                    "fieldValueLabel": "Restart_Cluster",
                    "sequence": 9,
                    "sysId": "0e4f6dc2baed4c1ebccf7ac23fb18967",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "terminate_cluster",
                    "fieldValueLabel": "Terminate Cluster",
                    "sequence": 10,
                    "sysId": "58bfe80ead4b40e4837584ee2b745aa3",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "get_cluster",
                    "fieldValueLabel": "Get a Cluster info",
                    "sequence": 11,
                    "sysId": "b150ab7541ef4b8ea62cd1d6a9fb6cbf",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "list_cluster",
                    "fieldValueLabel": "List Cluster",
                    "sequence": 12,
                    "sysId": "1faf794f30324100ad61e14936ebc545",
                    "useFieldValueForLabel": false
                },
                {
                    "fieldValue": "upload_dbfs_file",
                    "fieldValueLabel": "Upload local file to dbfs",
                    "sequence": 13,
                    "sysId": "75f291772921407c8bb4745020585332",
                    "useFieldValueForLabel": false
                }
            ],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Choice Field 1",
            "fieldType": "Choice",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Select a Function that would like to perform with Databricks",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "DataBricks Function",
            "name": "databricks_function",
            "noSpaceIfHidden": false,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": false,
            "required": false,
            "sequence": 2,
            "showIfField": null,
            "showIfFieldValue": null,
            "sysId": "bf109bb06baf494bb466c98faab3fd0d"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Script Field 1",
            "fieldType": "Script",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "feed the script for the new job creation or cluster in Databricks",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Create Request Script",
            "name": "create_request",
            "noSpaceIfHidden": true,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": true,
            "required": false,
            "sequence": 3,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "create_job,create_cluster",
            "sysId": "ce2f7965a3534cb4a47d177e56e4bf0a"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Text Field 2",
            "fieldType": "Text",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Provide the databricks Job ID",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Job ID",
            "name": "job_id",
            "noSpaceIfHidden": true,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": true,
            "required": false,
            "sequence": 4,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "delete_job,get_job_details,run_job_now",
            "sysId": "7f6a15adf3b14415a1030d5d8b0647d5"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Script Field 2",
            "fieldType": "Script",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Specify the parameters for Jar or notebook or python or spark-submit or the Job submit run request",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Job Run Request",
            "name": "job_params",
            "noSpaceIfHidden": true,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": true,
            "required": false,
            "sequence": 5,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "run_now_job,run_submit_job",
            "sysId": "00ecd06525b746da8254a26905cf69dd"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Text Field 3",
            "fieldType": "Text",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Specify the databricks Run ID",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Run ID",
            "name": "run_id",
            "noSpaceIfHidden": true,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": true,
            "required": false,
            "sequence": 6,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "cancel_job_run",
            "sysId": "01dfe8aff0bf4f06a7f04aa86af41bd0"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Text Field 4",
            "fieldType": "Text",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Provide the cluster ID",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Cluster ID ",
            "name": "cluster_id",
            "noSpaceIfHidden": false,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": false,
            "required": false,
            "sequence": 7,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "start_cluster,restart_cluster,terminate_cluster,get_cluster",
            "sysId": "a69de563164d4a5983b721f14c70b3cd"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Text Field 5",
            "fieldType": "Text",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Local file name with path",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "Local file name",
            "name": "local_file",
            "noSpaceIfHidden": true,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": true,
            "required": false,
            "sequence": 8,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "upload_dbfs_file",
            "sysId": "70f6d833a17f4d2d993897fb591d3b4d"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Text Field 6",
            "fieldType": "Text",
            "fieldValue": null,
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "provide the databricks file path and name",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "DBFS file name",
            "name": "dbfs_file",
            "noSpaceIfHidden": true,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": true,
            "required": false,
            "sequence": 9,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "upload_dbfs_file",
            "sysId": "dcfeef7de1284c529f8d16f6df28aa11"
        },
        {
            "arrayNameTitle": null,
            "arrayValueTitle": null,
            "booleanNoValue": null,
            "booleanValueType": "true/false",
            "booleanYesValue": null,
            "choiceAllowEmpty": false,
            "choiceAllowMultiple": false,
            "choiceSortOption": "Sequence",
            "choices": [],
            "defaultListView": false,
            "fieldLength": null,
            "fieldMapping": "Boolean Field 1",
            "fieldType": "Boolean",
            "fieldValue": "false",
            "formColumnSpan": 1,
            "formEndRow": false,
            "formStartRow": false,
            "hint": "Specify files need to overwritten in DBFS ",
            "intFieldMax": null,
            "intFieldMin": null,
            "label": "overwrite",
            "name": "overwrite",
            "noSpaceIfHidden": true,
            "requireIfField": null,
            "requireIfFieldValue": null,
            "requireIfVisible": false,
            "required": false,
            "sequence": 10,
            "showIfField": "Choice Field 1",
            "showIfFieldValue": "upload_dbfs_file",
            "sysId": "eb7748b7ab9641fdbe1c9b1ac77eef0f"
        }
    ],
    "name": "Databricks",
    "outputFailureOnly": false,
    "outputReturnFile": null,
    "outputReturnNline": "100",
    "outputReturnSline": "1",
    "outputReturnText": null,
    "outputReturnType": "NONE",
    "outputType": "STDOUT",
    "runtimeDir": null,
    "script": "#!/opt/universal/python3.6/bin/python3\n# --\n#         Origins: Stonebranch\n#          Author: Ravi Murugesan\n#            Date: 10-Nov-2020\n#\n#    Copyright (c) Stonebranch, 2019.  All rights reserved.\n#\n#         Purpose: This Task will Provide functionalities to integrate with Databricks for\n#                  Job and cluster functionalities                   \n#\nversion = \"1.0\"\n#           Version History:    0.1     Ravi Murugesan     11-Nov-2020     Initial Version\n#                               1.0     Ravi Murugesan     05-Feb-2021     Upload & Export functionalities\n#\n# --Import the required modules for execution\nimport argparse, logging, sys, uuid, time, datetime, json, requests,base64\n# -- loglevel settings INFO,DEBUG, WARNING, ERROR, CRITICAL\nlogging.basicConfig(level=logging.INFO, format=' %(asctime)s - %(levelname)s - %(message)s')\n# -- Main Logic Function\ndef main():\n    ScriptSetup()                                               # -- Import Required Modules, Setup Logging Format, Set Variables \n    if args.databricks_function == \"create_job\":\n        create_job()\n    elif args.databricks_function == \"list_jobs\":\n        list_jobs()\n    elif args.databricks_function == \"delete_job\":\n        delete_job()\n    elif args.databricks_function == \"get_job_details\":\n        get_job_details()\n    elif args.databricks_function == \"run_now_job\":\n        function_url=args.databricks_url+'/api/2.0/jobs/run-now'\n        run_job(function_url)\n    elif args.databricks_function == \"cancel_job_run\":\n        cancel_job_run()\n    elif args.databricks_function == \"run_submit_job\":\n        function_url=args.databricks_url+'/api/2.0/jobs/runs/submit'\n        run_job(function_url)\n    elif args.databricks_function == \"create_cluster\":\n        create_cluster()                                  \n    elif args.databricks_function == \"start_cluster\":\n        api_function=\"start-cluster\"\n        start_restart_terminate_cluster(api_function)\n    elif args.databricks_function == \"restart_cluster\":\n        api_function=\"restart-cluster\"\n        start_restart_terminate_cluster(api_function)\n    elif args.databricks_function == \"terminate_cluster\":\n        api_function=\"terminate-cluster\"\n        start_restart_terminate_cluster(api_function)\n    elif args.databricks_function == \"get_cluster\":\n        get_cluster()\n    elif args.databricks_function == \"list_cluster\":\n        list_cluster()\n    elif args.databricks_function == \"upload_dbfs_file\":\n        upload_dbfs_file()\n# --\n# -- Import Required Modules, Setup Logging Format, Set Variables                 \ndef ScriptSetup():\n    parser=argparse.ArgumentParser(description='Purpose : Databricks Jobs and cluster Orchestration')\n    \n    # ## --> Capture Universal Task Form Variables Here\n    parser.add_argument(\"--databricks_function\", default=\"${ops_DB_databricks_function}\")\n    parser.add_argument(\"--databricks_url\", default=\"${ops_DB_data_bricks_url}\")\n    parser.add_argument('--bearer_token', default=\"${_credentialPwd('${ops_DB_bearer_token}')}\")\n    parser.add_argument('--create_request', default='${ops_DB_create_request}')\n    parser.add_argument('--job_id',default='${ops_DB_job_id}')\n    parser.add_argument('--run_id',default='${ops_DB_run_id}')\n    parser.add_argument('--job_params',default='${ops_DB_job_params}')\n    parser.add_argument('--cluster_id',default='${ops_DB_cluster_id}')\n    parser.add_argument('--local_file',default='${ops_DB_local_file}')\n    parser.add_argument('--dbfs_file',default='${ops_DB_dbfs_file}')\n    parser.add_argument('--overwrite',default='${ops_DB_overwrite}')\n    # ## --\n    global args\n    args = parser.parse_args()\n    # ## --> Logging info\n    parser.add_argument(\"--logginglevel\", default=\"info\")\n    logging.info(\"Executing version \" + version + \" with the following paramaters\")\n    logging.info(args)\n# --\n#\n##############\n# This function is to upload local file to databricks file system\n##############\ndef dbfs_rpc(action, body):\n    \"\"\" A helper function to make the DBFS API request, request/response is encoded/decoded as JSON \"\"\"\n    try:\n        BASE_URL = args.databricks_url+'/api/2.0/dbfs/'\n        TOKEN = args.bearer_token\n        response = requests.post(\n            BASE_URL + action,\n            headers={'Authorization': 'Bearer %s' % TOKEN },\n            json=body\n            )\n        logging.info(response.text)\n        if response.status_code != 200:\n            print(\"Error:\",response.text)\n            sys.exit(1)\n        #print(response.status_code)\n        return response.json()\n    except Exception as e:\n        logging.error(\"Error dbfs_rpc function Action:\"+action)\n        logging.error(e)\n        logging.error(response.json)\n        sys.exit(1)\n# \n#\n#\ndef upload_dbfs_file():\n    try:\n        logging.info(\"Attempting to upload local file to Databricks Environment\")\n        #TOKEN = args.bearer_token\n        #BASE_URL = args.databricks_url+'/api/2.0/dbfs/'\n        # Create a handle that will be used to add blocks\n        handle = dbfs_rpc(\"create\", {\"path\": args.dbfs_file, \"overwrite\": args.overwrite})['handle']\n        with open(args.local_file,'rb') as f:\n            while True:\n                # A block can be at most 1MB\n                block = f.read(1 << 20)\n                if not block:\n                    break\n                data = base64.standard_b64encode(block)\n                dbfs_rpc(\"add-block\", {\"handle\": handle, \"data\": data.decode('UTF8')})\n    # close the handle to finish uploading\n        dbfs_rpc(\"close\", {\"handle\": handle})\n        logging.info(\"File :\"+args.local_file+\" Uploaded to \"+args.dbfs_file)\n    except Exception as e:\n        logging.error(\"Error in uploading file to Databricks file system\")\n        logging.error(e)\n        #logging.error(handle)\n        sys.exit(1)\n\n####\n#####\n# This function is to create job in Databricks based on the supplied json in universal controller\ndef create_job():\n    logging.info(\"Attempting Job Creation in Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/jobs/create'\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        'Content-Type':'application/json'\n        }\n    script = open(r'''${_scriptPath('${ops_DB_create_request}')}''', 'r')\n    script_data = script.read()\n    try:\n        response=requests.post(url,data=script_data,headers=header)\n    #    print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        format_response=response.json()\n        if (response.status_code != 200):\n            logging.error(\"Error - Job Creation in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n            logging.error(response.text)\n            sys.exit(1)\n        else:\n            logging.info(\"Job Created successfully in Databricks\")\n            logging.info(\"Job ID: \"+str(format_response['job_id']))\n            print(response.text)\n    except Exception as e:\n        logging.error(\"Error Creating jobs in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n\n############This function is to create Databricks Cluster \n#\ndef create_cluster():\n    logging.info(\"Attempting Create cluster in Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/clusters/create'\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        'Content-Type':'application/json'\n        }\n    script = open(r'''${_scriptPath('${ops_DB_create_request}')}''', 'r')\n    script_data = script.read()\n    try:\n        response=requests.post(url,data=script_data,headers=header)\n    #    print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        format_response=response.json()\n        if (response.status_code != 200):\n            logging.error(\"Error - cluster Creation in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n            logging.error(response.text)\n            sys.exit(1)\n        else:\n            logging.info(\"cluster Created successfully in Databricks\")\n            logging.info(\"cluster ID: \"+str(format_response['cluster_id']))\n            print(response.text)\n    except Exception as e:\n        logging.error(\"Error Creating Cluster in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n###########This function is to create is to start a cluster in Databricks environment\ndef start_restart_terminate_cluster(api_function):\n    if(api_function==\"start-cluster\"):\n        url=url=args.databricks_url+'/api/2.0/clusters/start'\n        mode=\"start\"\n    elif(api_function==\"restart-cluster\"):\n        url=args.databricks_url+'/api/2.0/clusters/restart'\n        mode=\"restart\"\n    elif(api_function==\"terminate-cluster\"):\n        url=args.databricks_url+'/api/2.0/clusters/delete'\n        mode=\"terminate\"\n\n    logging.info(\"Attempting to \"+mode+ \" in Databricks Environment\")\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        'Content-Type':'application/json'\n        }\n    body = {\"cluster_id\": args.cluster_id }\n    try:\n        response=requests.post(url,json.dumps(body),headers=header)\n        print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        if (response.status_code != 200):\n            logging.error(\"Error - \"+mode+\" a cluster in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n            logging.error(response.text)\n            sys.exit(1)\n        else:\n            logging.info(\"Cluster ID : \"+str(args.cluster_id)+\" \"+mode+\" successfully in Databricks\")\n            print(response.text)\n    except Exception as e:\n        logging.error(\"Error \"+mode+\" cluster in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n########This function is to get the cluster definition details\ndef get_cluster():\n    logging.info(\"Get cluster details from Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/clusters/get?cluster_id='+args.cluster_id\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        }\n    try:\n        response=requests.get(url,headers=header)\n    #    print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        format_response=response.json()\n        if (response.status_code != 200):\n            logging.error(\"Error getting cluster details in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n            sys.exit(1)\n        else:\n            logging.info(\"Printed cluster details in STDOUT\")\n            print(format_response)\n    except Exception as e:\n        logging.error(\"Error getting cluster details in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n####This function is to get the clusters that are available in DataBricks environment\ndef list_cluster():\n    logging.info(\"Listing all cluster in Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/clusters/list'\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        }\n    try:\n        response=requests.get(url,headers=header)\n        #print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        format_response=response.json()\n        if (response.status_code != 200):\n            logging.error(\"Error in listing clusters in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n        else:\n            logging.info(\"|-----------------------------|\")\n            logging.info(\"|---Listing Cluster Name: ----|\")\n            logging.info(\"|-----------------------------|\")\n            jobs_list_number=len(format_response)\n            if jobs_list_number == 0:\n                logging.info(\"No clusters in your Databricks Environment\")\n            else:\n                for clusters in format_response['clusters']:\n                    cluster_id=clusters['cluster_id']\n                    cluster_name=clusters['default_tags']['ClusterName']\n                    cluster_state=clusters['state']\n                    logging.info(str(cluster_id)+\" ==>\"+cluster_name+\" ==>\"+cluster_state)\n                #print(response.text)\n    except Exception as e:\n        logging.error(\"Error listing clusters in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n\n\n###########This function is to cancel the job that is currently executing in Databricks\ndef cancel_job_run():\n    logging.info(\"Attempting Cancel job execution in Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/jobs/runs/cancel'\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        'Content-Type':'application/json'\n        }\n    body = {\"run_id\": args.run_id }\n    try:\n        response=requests.post(url,json.dumps(body),headers=header)\n        print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        if (response.status_code != 200):\n            logging.error(\"Error - Job cancellation in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n            logging.error(response.text)\n            sys.exit(1)\n        else:\n            logging.info(\"Run ID : \"+str(args.run_id)+\" Cancelled successfully in Databricks\")\n            print(response.text)\n    except Exception as e:\n        logging.error(\"Error Cancelling jobs in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n\n####This function is to list the jobs that are available in DataBricks environment\ndef list_jobs():\n    logging.info(\"Listing jobs in Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/jobs/list'\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        }\n    try:\n        response=requests.get(url,headers=header)\n    #    print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        format_response=response.json()\n        if (response.status_code != 200):\n            logging.error(\"Error in listing jobs in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n        else:\n            logging.info(\"|-------------------------|\")\n            logging.info(\"|---Listing Job Name: ----|\")\n            logging.info(\"|-------------------------|\")\n            jobs_list_number=len(format_response)\n            if jobs_list_number == 0:\n                logging.info(\"No Jobs in your Databricks Environment\")\n            else:\n                for jobs in format_response['jobs']:\n                    job_id=jobs['job_id']\n                    job_name=jobs['settings']['name']\n                    logging.info(str(job_id)+\" ==>\"+job_name)\n                print(response.text)\n    except Exception as e:\n        logging.error(\"Error listing jobs in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n\n########This function is to get the existing JOb definition details\ndef get_job_details():\n    logging.info(\"Get job details from Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/jobs/get?job_id='+args.job_id\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        }\n    try:\n        response=requests.get(url,headers=header)\n    #    print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        format_response=response.json()\n        if (response.status_code != 200):\n            logging.error(\"Error in listing jobs in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n            sys.exit(1)\n        else:\n            logging.info(\"Printed Job details in STDOUT\")\n            print(format_response)\n    except Exception as e:\n        logging.error(\"Error listing jobs in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n##########This function is to delete the job in Azure Databricks\ndef delete_job():\n    logging.info(\"Attempting deletion of job in Databricks Environment\")\n    url=args.databricks_url+'/api/2.0/jobs/delete'\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        'Content-Type':'application/json'\n        }\n    body = {\"job_id\": args.job_id }\n    try:\n        response=requests.post(url,json.dumps(body),headers=header)\n    #    print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        if (response.status_code != 200):\n            logging.error(\"Error - Job Deletion in Databricks \")\n            logging.error(\"API Error code: \"+str(response.status_code))\n            logging.error(response.text)\n            sys.exit(1)\n        else:\n            logging.info(\"Job ID : \"+str(args.job_id)+\" Deleted successfully in Databricks\")\n            print(response.text)\n    except Exception as e:\n        logging.error(\"Error Deleting jobs in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n###############This is run a job now in Databricks environment\n# \ndef run_job(function_url):\n    logging.info(\"Attempting Job Execution in Databricks Environment\")\n    url=function_url\n    logging.info(\"Databricks URL: \"+url)\n    header={\n        'Authorization':'Bearer '+args.bearer_token,\n        'Content-Type':'application/json'\n        }\n    script = open(r'''${_scriptPath('${ops_DB_job_params}')}''', 'r')\n    script_data = script.read()\n    try:\n        response=requests.post(url,data=script_data,headers=header)\n        format_response=response.json()\n    #    print(response.text)\n        logging.info(\"DataBricks API Return code: \"+str(response.status_code))\n        if (response.status_code != 200):\n            logging.error(\"Error - Triggering Execution of job in Databricks \")\n            logging.error(response.text)\n            sys.exit(1)\n        else:\n            run_id=format_response['run_id']\n            logging.info(\"Run ID of this execution : \"+str(run_id))\n            print(response.text)\n            check_status=True\n            get_run_url=args.databricks_url+'/api/2.0/jobs/runs/get?run_id='+str(run_id)\n            while check_status:\n                try:\n                    check_run_status=requests.get(get_run_url,headers=header)\n                    #print(check_run_status.text)\n                    format_response=check_run_status.json()\n                    if(check_run_status.status_code ==200):\n                        life_cycle_state=format_response['state']['life_cycle_state']\n                        if(life_cycle_state==\"TERMINATED\" or life_cycle_state==\"SKIPPED\" or life_cycle_state==\"INTERNAL_ERROR\"):\n                            result_state=format_response['state']['result_state']\n                            logging.info(\"job execution completed with status: \"+result_state)\n                            logging.info(check_run_status.text)\n                            get_output_url=args.databricks_url+'/api/2.0/jobs/runs/get-output?run_id='+str(run_id)\n                            try:\n                                logging.info(\"Attempting to fetch Output logs\")\n                                logs_response=requests.get(get_output_url,headers=header)\n                                format_logs_response=logs_response.json()\n                                #output_logs=format_logs_response['notebook_output']\n                                if(logs_response.status_code !=200):\n                                    logging.error(\"Error in Fetching the logs\")\n                                    logging.error(logs_response.text)\n                                else:\n                                    if(result_state !=\"SUCCESS\"):\n                                        logging.error(logs_response.text)\n                                        sys.exit(1)\n                                    else:\n                                        logging.info(\"**********Output Logs**********\")\n                                        logging.info(logs_response.text)\n                                        print(format_logs_response['notebook_output'])\n                            except Exception as e:\n                                logging.error(\"Error in fetching the logs for Job\")\n                                logging.error(logs_response.text)\n                                sys.exit(1)\n                            ###Change the flag status and break the loop\n                            check_status=False\n                            break\n                        else:\n                            #####poll for every 10 seconds\"\n                            time.sleep(10)\n                    else:\n                        logging.error(\" Fetching Run ID Errored : \"+check_run_status.text)\n                        logging.error(\"Return code: \"+str(check_run_status.status_code))\n\n                except Exception as e:\n                    logging.error(\"Error Monitoring Databricks RunNow execution\")\n                    logging.error(\"error response: \"+check_run_status.text)\n                    logging.error(e)\n                    sys.exit(1)\n    except Exception as e:\n        logging.error(\"Error Executing jobs in Databricks\")\n        logging.error(e)\n        sys.exit(1)\n\n# ## --> Functions Go Here\n# --\n# -- Execute\nmain()",
    "scriptTypeWindows": "uapy",
    "scriptUnix": null,
    "scriptWindows": null,
    "sysId": "2ed836cc9f674c1cb7cf8f4fa400161c",
    "useCommonScript": true,
    "variablePrefix": "DB",
    "waitForOutput": false
}